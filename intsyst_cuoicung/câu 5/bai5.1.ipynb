{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo thành công hai tập dữ liệu thử nghiệm.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Khởi tạo sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Dữ liệu mẫu với các comment giả định\n",
    "data = {\n",
    "    'product_id': [f'P{random.randint(100, 200)}' for _ in range(20)],\n",
    "    'comment': [\n",
    "        \"Sản phẩm rất tốt, mình rất hài lòng.\",\n",
    "        \"Không như mong đợi, sản phẩm quá tệ.\",\n",
    "        \"Mình thích thiết kế của sản phẩm này!\",\n",
    "        \"Chất lượng tạm ổn, giá hợp lý.\",\n",
    "        \"Giao hàng nhanh, sản phẩm đúng như quảng cáo.\",\n",
    "        \"Màu sắc không giống trong ảnh, thất vọng.\",\n",
    "        \"Giá cả phải chăng, đáng mua.\",\n",
    "        \"Đóng gói cẩn thận, nhưng chất lượng không tốt lắm.\",\n",
    "        \"Hài lòng với sản phẩm này, sẽ mua lần nữa.\",\n",
    "        \"Sản phẩm lỗi, không khuyến khích mua.\",\n",
    "        \"Đẹp và chất lượng tốt!\",\n",
    "        \"Quá đắt so với chất lượng nhận được.\",\n",
    "        \"Mình sẽ giới thiệu sản phẩm này cho bạn bè.\",\n",
    "        \"Hàng về bị móp, không hài lòng.\",\n",
    "        \"Dịch vụ chăm sóc khách hàng kém.\",\n",
    "        \"Nhận hàng nhanh chóng và chất lượng tuyệt vời.\",\n",
    "        \"Màu sắc hợp thời trang.\",\n",
    "        \"Không nên mua, chất lượng kém.\",\n",
    "        \"Phù hợp với giá tiền, mình thấy ổn.\",\n",
    "        \"Dịch vụ tuyệt vời và sản phẩm chất lượng.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Tạo DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Gán nhãn sentiment\n",
    "def label_sentiment(comment):\n",
    "    score = sia.polarity_scores(comment)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'Tích cực'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'Tiêu cực'\n",
    "    else:\n",
    "        return 'Trung lập'\n",
    "\n",
    "df['sentiment'] = df['comment'].apply(label_sentiment)\n",
    "\n",
    "# Xuất dữ liệu ra tập dữ liệu đầu tiên - Dữ liệu gốc\n",
    "df_real_data = df.copy()\n",
    "df_real_data.to_csv('real_data.csv', index=False)\n",
    "\n",
    "# Tạo tập dữ liệu 2: Gán nhãn thủ công theo xu hướng\n",
    "def assign_preference(comment):\n",
    "    if 'thích' in comment or 'hài lòng' in comment:\n",
    "        return 'Sở thích'\n",
    "    elif 'không' in comment or 'tệ' in comment or 'thất vọng' in comment:\n",
    "        return 'Không thích'\n",
    "    else:\n",
    "        return 'Trung lập'\n",
    "\n",
    "df['preference'] = df['comment'].apply(assign_preference)\n",
    "\n",
    "# Xuất dữ liệu ra tập dữ liệu thứ hai - Dữ liệu gán nhãn\n",
    "df_annotated_data = df.copy()\n",
    "df_annotated_data.to_csv('annotated_data.csv', index=False)\n",
    "\n",
    "print(\"Đã tạo thành công hai tập dữ liệu thử nghiệm.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vncorenlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvncorenlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VnCoreNLP\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vncorenlp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from vncorenlp import VnCoreNLP\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Khởi tạo sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Khởi tạo VnCoreNLP\n",
    "annotator = VnCoreNLP(\"VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "\n",
    "# Đọc dữ liệu từ file\n",
    "df = pd.read_csv('real_data.csv')\n",
    "\n",
    "# 1. Loại bỏ các ký tự đặc biệt và chữ số\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Loại bỏ số\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Loại bỏ dấu câu\n",
    "    return text\n",
    "\n",
    "df['cleaned_comment'] = df['comment'].apply(remove_special_characters)\n",
    "\n",
    "# 2. Chuyển đổi thành chữ thường\n",
    "df['cleaned_comment'] = df['cleaned_comment'].str.lower()\n",
    "\n",
    "# 3. Định nghĩa bộ stopwords tiếng Việt\n",
    "stop_words = set([\n",
    "    \"và\", \"là\", \"của\", \"có\", \"cho\", \"với\", \"từ\", \"như\", \"nhưng\", \"về\", \"mình\", \"này\", \"đó\", \"ở\", \"thì\", \"làm\", \n",
    "    \"nên\", \"ra\", \"khi\", \"để\", \"nếu\", \"sẽ\", \"các\", \"còn\", \"đến\", \"đang\", \"tại\", \"hay\", \"cùng\", \"hoặc\"\n",
    "    # Bổ sung thêm các từ stopwords tiếng Việt khác nếu cần\n",
    "])\n",
    "\n",
    "# 4. Loại bỏ stopwords và tách từ\n",
    "def remove_stopwords(text):\n",
    "    # Tách từ với VnCoreNLP\n",
    "    words = annotator.tokenize(text)\n",
    "    words = [w for sentence in words for w in sentence]  # Làm phẳng danh sách\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(remove_stopwords)\n",
    "\n",
    "# 5. Gán nhãn sentiment cho comment đã được tiền xử lý\n",
    "def label_sentiment(text):\n",
    "    score = sia.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'Tích cực'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'Tiêu cực'\n",
    "    else:\n",
    "        return 'Trung lập'\n",
    "\n",
    "df['sentiment'] = df['cleaned_comment'].apply(label_sentiment)\n",
    "\n",
    "# 6. Lưu dữ liệu đã tiền xử lý\n",
    "df.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "print(\"Tiền xử lý dữ liệu hoàn tất và đã lưu vào preprocessed_data.csv\")\n",
    "\n",
    "# Đóng annotator sau khi sử dụng\n",
    "annotator.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
